{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget, os, gzip, pickle, random, re, sys\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import torch.distributions as dist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB_URL = 'http://dlvu.github.io/data/imdb.{}.pkl.gz'\n",
    "IMDB_FILE = 'imdb.{}.pkl.gz'\n",
    "\n",
    "PAD, START, END, UNK = '.pad', '.start', '.end', '.unk'\n",
    "\n",
    "def load_imdb(final=False, val=5000, seed=0, voc=None, char=False):\n",
    "\n",
    "    cst = 'char' if char else 'word'\n",
    "\n",
    "    imdb_url = IMDB_URL.format(cst)\n",
    "    imdb_file = IMDB_FILE.format(cst)\n",
    "\n",
    "    if not os.path.exists(imdb_file):\n",
    "        wget.download(imdb_url)\n",
    "\n",
    "    with gzip.open(imdb_file) as file:\n",
    "        sequences, labels, i2w, w2i = pickle.load(file)\n",
    "\n",
    "    if voc is not None and voc < len(i2w):\n",
    "        nw_sequences = {}\n",
    "\n",
    "        i2w = i2w[:voc]\n",
    "        w2i = {w: i for i, w in enumerate(i2w)}\n",
    "\n",
    "        mx, unk = voc, w2i['.unk']\n",
    "        for key, seqs in sequences.items():\n",
    "            nw_sequences[key] = []\n",
    "            for seq in seqs:\n",
    "                seq = [s if s < mx else unk for s in seq]\n",
    "                nw_sequences[key].append(seq)\n",
    "\n",
    "        sequences = nw_sequences\n",
    "\n",
    "    if final:\n",
    "        return (sequences['train'], labels['train']), (sequences['test'], labels['test']), (i2w, w2i), 2\n",
    "\n",
    "    # Make a validation split\n",
    "    random.seed(seed)\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    x_val, y_val = [], []\n",
    "\n",
    "    val_ind = set( random.sample(range(len(sequences['train'])), k=val) )\n",
    "    for i, (s, l) in enumerate(zip(sequences['train'], labels['train'])):\n",
    "        if i in val_ind:\n",
    "            x_val.append(s)\n",
    "            y_val.append(l)\n",
    "        else:\n",
    "            x_train.append(s)\n",
    "            y_train.append(l)\n",
    "\n",
    "    return (x_train, y_train), \\\n",
    "           (x_val, y_val), \\\n",
    "           (i2w, w2i), 2\n",
    "\n",
    "\n",
    "def gen_sentence(sent, g):\n",
    "\n",
    "    symb = '_[a-z]*'\n",
    "\n",
    "    while True:\n",
    "\n",
    "        match = re.search(symb, sent)\n",
    "        if match is None:\n",
    "            return sent\n",
    "\n",
    "        s = match.span()\n",
    "        sent = sent[:s[0]] + random.choice(g[sent[s[0]:s[1]]]) + sent[s[1]:]\n",
    "\n",
    "def gen_dyck(p):\n",
    "    open = 1\n",
    "    sent = '('\n",
    "    while open > 0:\n",
    "        if random.random() < p:\n",
    "            sent += '('\n",
    "            open += 1\n",
    "        else:\n",
    "            sent += ')'\n",
    "            open -= 1\n",
    "\n",
    "    return sent\n",
    "\n",
    "def gen_ndfa(p):\n",
    "\n",
    "    word = random.choice(['abc!', 'uvw!', 'klm!'])\n",
    "\n",
    "    s = ''\n",
    "    while True:\n",
    "        if random.random() < p:\n",
    "            return 's' + s + 's'\n",
    "        else:\n",
    "            s+= word\n",
    "\n",
    "def load_brackets(n=50_000, seed=0):\n",
    "    return load_toy(n, char=True, seed=seed, name='dyck')\n",
    "\n",
    "def load_ndfa(n=50_000, seed=0):\n",
    "    return load_toy(n, char=True, seed=seed, name='ndfa')\n",
    "\n",
    "def load_toy(n=50_000, char=True, seed=0, name='lang'):\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    if name == 'lang':\n",
    "        sent = '_s'\n",
    "\n",
    "        toy = {\n",
    "            '_s': ['_s _adv', '_np _vp', '_np _vp _prep _np', '_np _vp ( _prep _np )', '_np _vp _con _s' , '_np _vp ( _con _s )'],\n",
    "            '_adv': ['briefly', 'quickly', 'impatiently'],\n",
    "            '_np': ['a _noun', 'the _noun', 'a _adj _noun', 'the _adj _noun'],\n",
    "            '_prep': ['on', 'with', 'to'],\n",
    "            '_con' : ['while', 'but'],\n",
    "            '_noun': ['mouse', 'bunny', 'cat', 'dog', 'man', 'woman', 'person'],\n",
    "            '_vp': ['walked', 'walks', 'ran', 'runs', 'goes', 'went'],\n",
    "            '_adj': ['short', 'quick', 'busy', 'nice', 'gorgeous']\n",
    "        }\n",
    "\n",
    "        sentences = [ gen_sentence(sent, toy) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s : len(s))\n",
    "\n",
    "    elif name == 'dyck':\n",
    "\n",
    "        sentences = [gen_dyck(7./16.) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s: len(s))\n",
    "\n",
    "    elif name == 'ndfa':\n",
    "\n",
    "        sentences = [gen_ndfa(1./4.) for _ in range(n)]\n",
    "        sentences.sort(key=lambda s: len(s))\n",
    "\n",
    "    else:\n",
    "        raise Exception(name)\n",
    "\n",
    "    tokens = set()\n",
    "    for s in sentences:\n",
    "\n",
    "        if char:\n",
    "            for c in s:\n",
    "                tokens.add(c)\n",
    "        else:\n",
    "            for w in s.split():\n",
    "                tokens.add(w)\n",
    "\n",
    "    i2t = [PAD, START, END, UNK] + list(tokens)\n",
    "    t2i = {t:i for i, t in enumerate(i2t)}\n",
    "\n",
    "    sequences = []\n",
    "    for s in sentences:\n",
    "        if char:\n",
    "            tok = list(s)\n",
    "        else:\n",
    "            tok = s.split()\n",
    "        sequences.append([t2i[t] for t in tok])\n",
    "\n",
    "    return sequences, (i2t, t2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prep_data():\n",
    "    def __init__(self, dataset, n=50000, max_tokens=150):\n",
    "        self.dataset = dataset\n",
    "        self.n_instances = n\n",
    "        self.max_tokens = max_tokens\n",
    "        self.n_batches = math.ceil(self.n_instances/self.max_tokens)\n",
    "        \n",
    "    \n",
    "    def load_data(self):\n",
    "        \n",
    "        if self.dataset == 'ndfa':\n",
    "            self.x_train, (self.i2w, self.w2i) = load_ndfa(n=self.n_instances)\n",
    "            self.seeds = [[self.w2i['.start'], self.w2i['s'],  self.w2i['a']],\n",
    "                          [self.w2i['.start'], self.w2i['s'],  self.w2i['k']],\n",
    "                          [self.w2i['.start'], self.w2i['s'],  self.w2i['u']],\n",
    "                          [self.w2i['.start'], self.w2i['s']],\n",
    "                          [self.w2i['.start']]]\n",
    "\n",
    "        elif self.dataset == 'brackets':\n",
    "            self.x_train, (self.i2w, self.w2i) = load_brackets(n=self.n_instances)\n",
    "            self.seeds = [[self.w2i['.start'], self.w2i['('],  self.w2i['('], self.w2i['(']],\n",
    "                          [self.w2i['.start'], self.w2i['('],  self.w2i['('], self.w2i[')']],\n",
    "                          [self.w2i['.start'], self.w2i['('],  self.w2i['(']],\n",
    "                          [self.w2i['.start'], self.w2i['(']],\n",
    "                          [self.w2i['.start']]]\n",
    "        \n",
    "        elif self.dataset == 'toy':\n",
    "            self.x_train, (self.i2w, self.w2i) = load_toy(n=self.n_instances)\n",
    "            self.seeds = [self.w2i['.start']]\n",
    "        \n",
    "        elif self.dataset == 'imdb':\n",
    "            (self.x_train, y_train), (x_val, y_val), (self.i2w, self.w2i), self.numcls = load_imdb(final=False, char=True)\n",
    "            self.seeds = [self.w2i['.start']]\n",
    "        \n",
    "        np.random.shuffle(self.x_train)\n",
    "\n",
    "            \n",
    "        \n",
    "    def tokens_in_batch(self, fr, to):\n",
    "        n_tokens = 0\n",
    "\n",
    "        for i in range(fr, to):\n",
    "            n_tokens += len(self.x_train[i])\n",
    "\n",
    "        return n_tokens\n",
    "    \n",
    "    \n",
    "    def prep_batch(self, batch):\n",
    "        max_length = max([len(i) for i in batch]) + 2\n",
    "        new_batch = torch.empty((len(batch), max_length), dtype=torch.long)\n",
    "        lengths = torch.empty(len(batch), dtype=torch.long)\n",
    "\n",
    "        for i, instance in enumerate(batch):\n",
    "            instance.insert(0, self.w2i['.start'])\n",
    "            instance.append(self.w2i['.end'])\n",
    "            lengths[i] = len(instance)\n",
    "            \n",
    "            while len(instance) < max_length:\n",
    "                instance.append(self.w2i['.pad'])\n",
    "\n",
    "            new_batch[i] = torch.LongTensor(instance)\n",
    "\n",
    "        return new_batch, lengths\n",
    "                \n",
    "        \n",
    "    def get_batches(self):\n",
    "        self.load_data()\n",
    "        batches, lengths, targets = [], [], []\n",
    "        fr, to = 0, 0\n",
    "        \n",
    "        while to < self.n_instances:\n",
    "            to = fr\n",
    "            while self.tokens_in_batch(fr, to) < self.max_tokens:\n",
    "                to = min(to + 1, self.n_instances)\n",
    "                \n",
    "                if to == self.n_instances:\n",
    "                    break\n",
    "                    \n",
    "            batch, length = self.prep_batch(self.x_train[fr:to])\n",
    "            batches.append(batch)\n",
    "            lengths.append(length)\n",
    "            \n",
    "            target = np.c_[batch[:, 1:], np.zeros(batch.shape[0])]            \n",
    "            targets.append(torch.LongTensor(target))\n",
    "            \n",
    "            fr = to\n",
    "        \n",
    "        return batches, lengths, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, dataset, hidden_layer_size=16, embedding_dimension=32, num_layers=1, max_tokens=150, n=150000):\n",
    "        super(RNN, self).__init__()\n",
    "        self.data_loader = Prep_data(dataset, n=n, max_tokens=max_tokens)\n",
    "        self.X, self.lengths, self.Y = self.data_loader.get_batches()\n",
    "        self.criterion = nn.CrossEntropyLoss() \n",
    "        self.hidden_layer_size, self.num_layers = hidden_layer_size, num_layers\n",
    "\n",
    "        self.emb = nn.Embedding(len(self.data_loader.i2w), embedding_dimension, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dimension, self.hidden_layer_size, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(self.hidden_layer_size, len(self.data_loader.i2w))\n",
    "\n",
    "        \n",
    "    def forward(self, x, length):\n",
    "        hidden_cell = (torch.randn(self.num_layers, len(x), self.hidden_layer_size).to(device),\n",
    "                       torch.randn(self.num_layers, len(x), self.hidden_layer_size).to(device))\n",
    "        \n",
    "        x = self.emb(x)\n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, length, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        x, hidden_cell = self.lstm(x, hidden_cell)\n",
    "        x, sizes = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "       \n",
    "    def train(self, optimizer, device, epochs=3, max_length=50):\n",
    "        self.device = device\n",
    "\n",
    "        trainloss = np.array([])\n",
    "        dataset = list(zip(self.X, self.lengths, self.Y))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(dataset)\n",
    "\n",
    "            for i, (x, length, y) in enumerate(dataset, 0):\n",
    "                inputs, labels = x.to(self.device), y.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.forward(inputs, length)\n",
    "                loss = self.criterion(outputs.permute(0, 2, 1), labels)\n",
    "                loss.backward()\n",
    "                \n",
    "                nn.utils.clip_grad_value_(self.parameters(), 1)\n",
    "                optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                epochloss = 0\n",
    "                for i, (x, length, y) in enumerate(dataset, 0):\n",
    "                    inputs, labels = x.to(self.device), y.to(self.device)\n",
    "                    outputs = self.forward(inputs, length)\n",
    "                    batchloss = self.criterion(outputs.permute(0, 2, 1), labels).to('cpu')/len(x)\n",
    "                    trainloss = np.append(trainloss, batchloss)\n",
    "                    epochloss += batchloss\n",
    "\n",
    "            print(f'EPOCH {epoch} ||')\n",
    "            print(f'\\tTRAIN: {epochloss/len(dataset)} ||')\n",
    "            \n",
    "            for i in range(10):\n",
    "                self.generate(max_length)\n",
    "            print()\n",
    "\n",
    "        return np.convolve(trainloss, np.ones(50), 'valid') / 50\n",
    "\n",
    "\n",
    "    def sample(self, lnprobs, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Sample an element from a categorical distribution\n",
    "        :param lnprobs: Outcome logits\n",
    "        :param temperature: Sampling temperature. 1.0 follows the given\n",
    "        distribution, 0.0 returns the maximum probability element.\n",
    "        :return: The index of the sampled element.\n",
    "        \"\"\"\n",
    "        if temperature == 0.0:\n",
    "            return lnprobs.argmax()\n",
    "\n",
    "        p = F.softmax(lnprobs / temperature, dim=0)\n",
    "        cd = dist.Categorical(p)\n",
    "\n",
    "        return cd.sample()\n",
    "\n",
    "\n",
    "    def generate(self, max_length=50):\n",
    "        if len(self.data_loader.seeds) == 1:\n",
    "            seed = copy.deepcopy(self.data_loader.seeds)\n",
    "        else:\n",
    "            seed = copy.deepcopy(np.random.choice(self.data_loader.seeds))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i in range(max_length):\n",
    "                probs = self.forward(torch.LongTensor([seed]).to(self.device), [len(seed)])\n",
    "                token = self.sample(probs[-1, -1, :])\n",
    "                seed.append(token.item())\n",
    "\n",
    "                if token == self.data_loader.w2i['.end']:\n",
    "                    break\n",
    "                    \n",
    "        print(''.join([self.data_loader.i2w[i] for i in seed]) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN A MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = RNN('toy', hidden_layer_size=32, embedding_dimension=32, num_layers=2, max_tokens=400, n=1000)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "mod.to(device)\n",
    "optimizer = optim.Adam(mod.parameters(), lr = 0.00025)\n",
    "len(mod.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss = mod.train(optimizer, device, epochs=15, max_length=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndfa_loss = np.zeros((5, 6000))\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(f'REPETITION: {i}')\n",
    "    mod = RNN('ndfa', hidden_layer_size=16, embedding_dimension=32, num_layers=1, max_tokens=500, n=15000)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    mod.to(device)\n",
    "    optimizer = optim.Adam(mod.parameters(), lr = 0.0005)\n",
    "    ndfa_loss[i, :] = mod.train(optimizer, device, epochs=15, max_length=150)[:6000]\n",
    "\n",
    "    \n",
    "t = np.arange(1, 6001)\n",
    "plt.plot(t, ndfa_loss.mean(axis=0), lw=2, label='Sigmoid', color='blue')\n",
    "plt.fill_between(t, ndfa_loss.mean(axis=0) + ndfa_loss.std(axis=0), \n",
    "                 ndfa_loss.mean(axis=0) - ndfa_loss.std(axis=0), \n",
    "                 facecolor='blue', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.grid()\n",
    "\n",
    "#plt.savefig('ndfa')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRACKETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bracket_loss = np.zeros((5, 4650))\n",
    "for i in range(5):\n",
    "    print()\n",
    "    print(f'REPETITION: {i}')\n",
    "    mod = RNN('brackets', hidden_layer_size=16, embedding_dimension=32, num_layers=1, max_tokens=400, n=15000)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    mod.to(device)\n",
    "    optimizer = optim.Adam(mod.parameters(), lr = 0.0005)\n",
    "    bracket_loss[i, :] = mod.train(optimizer, device, epochs=15, max_length=150)[:4650]\n",
    "\n",
    "    \n",
    "t = np.arange(1, 4651)\n",
    "plt.plot(t, bracket_loss.mean(axis=0), lw=2, label='Sigmoid', color='blue')\n",
    "plt.fill_between(t, bracket_loss.mean(axis=0) + bracket_loss.std(axis=0), \n",
    "                 bracket_loss.mean(axis=0) - bracket_loss.std(axis=0), \n",
    "                 facecolor='blue', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.grid()\n",
    "\n",
    "#plt.savefig('bracket')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toy_loss = np.zeros((3, 31800))\n",
    "for i in range(3):\n",
    "    print()\n",
    "    print(f'REPETITION: {i}')\n",
    "    mod = RNN('toy', hidden_layer_size=32, embedding_dimension=32, num_layers=2, max_tokens=400, n=20000)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    mod.to(device)\n",
    "    optimizer = optim.Adam(mod.parameters(), lr = 0.00025)\n",
    "    toy_loss[i, :] = mod.train(optimizer, device, epochs=15, max_length=150)[:31800]\n",
    "\n",
    "    \n",
    "t = np.arange(1, 31801)\n",
    "plt.plot(t, toy_loss.mean(axis=0), lw=2, label='Sigmoid', color='blue')\n",
    "plt.fill_between(t, toy_loss.mean(axis=0) + toy_loss.std(axis=0), \n",
    "                 toy_loss.mean(axis=0) - toy_loss.std(axis=0), \n",
    "                 facecolor='blue', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.grid()\n",
    "\n",
    "#plt.savefig('toy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
