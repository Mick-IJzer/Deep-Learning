{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib import request\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_synth(num_train=60_000, num_val=10_000):\n",
    "    \"\"\"\n",
    "    Load some very basic synthetic data that should be easy to classify. Two features, so that we can plot the\n",
    "    decision boundary (which is an ellipse in the feature space).\n",
    "    :param num_train: Number of training instances\n",
    "    :param num_val: Number of test/validation instances\n",
    "    :param num_features: Number of features per instance\n",
    "    :return: Two tuples (xtrain, ytrain), (xval, yval) the training data is a floating point numpy array:\n",
    "    \"\"\"\n",
    "\n",
    "    THRESHOLD = 0.6\n",
    "    quad = np.asarray([[1, 0.5], [1, .2]])\n",
    "\n",
    "    ntotal = num_train + num_val\n",
    "\n",
    "    x = np.random.randn(ntotal, 2)\n",
    "\n",
    "    # compute the quadratic form\n",
    "    q = np.einsum('bf, fk, bk -> b', x, quad, x)\n",
    "    y = (q > THRESHOLD).astype(np.int)\n",
    "\n",
    "    return (x[:num_train, :], y[:num_train]), (x[num_train:, :], y[num_train:]), 2\n",
    "\n",
    "\n",
    "def load_mnist(final=False, flatten=True):\n",
    "    \"\"\"\n",
    "    Load the MNIST data\n",
    "    :param final: If true, return the canonical test/train split. If false, split some validation data from the training\n",
    "       data and keep the test data hidden.\n",
    "    :param flatten:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.isfile('mnist.pkl'):\n",
    "        init()\n",
    "\n",
    "    xtrain, ytrain, xtest, ytest = load()\n",
    "    xtl, xsl = xtrain.shape[0], xtest.shape[0]\n",
    "\n",
    "    if flatten:\n",
    "        xtrain = xtrain.reshape(xtl, -1)\n",
    "        xtest  = xtest.reshape(xsl, -1)\n",
    "\n",
    "    if not final: # return the flattened images\n",
    "        return (xtrain[:-5000], ytrain[:-5000]), (xtrain[-5000:], ytrain[-5000:]), 10\n",
    "\n",
    "    return (xtrain, ytrain), (xtest, ytest), 10\n",
    "\n",
    "# Numpy-only MNIST loader. Courtesy of Hyeonseok Jung\n",
    "# https://github.com/hsjeong5/MNIST-for-Numpy\n",
    "\n",
    "filename = [\n",
    "[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
    "[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
    "[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
    "[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
    "]\n",
    "\n",
    "def download_mnist():\n",
    "    base_url = \"http://yann.lecun.com/exdb/mnist/\"\n",
    "    for name in filename:\n",
    "        print(\"Downloading \"+name[1]+\"...\")\n",
    "        request.urlretrieve(base_url+name[1], name[1])\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "def save_mnist():\n",
    "    mnist = {}\n",
    "    for name in filename[:2]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n",
    "    for name in filename[-2:]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    with open(\"mnist.pkl\", 'wb') as f:\n",
    "        pickle.dump(mnist,f)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "def init():\n",
    "    download_mnist()\n",
    "    save_mnist()\n",
    "\n",
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_net:\n",
    "    '''\n",
    "    Neural network with 2 inputs, \n",
    "    1 hidden layer with 3 nodes and sigmoid activation, \n",
    "    an output layer with 2 nodes and softmax activation.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.w1 = [[0, 0, 0],\n",
    "              [0, 0, 0]]\n",
    "        self.b1 = [0, 0, 0]\n",
    "        self.w2 = [[0, 0],\n",
    "                  [0, 0],\n",
    "                  [0, 0]]\n",
    "        self.b2 = [0, 0]\n",
    "        self.lr = 0.05\n",
    "        self.epochs = 5\n",
    "        \n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        for i in range(len(self.w1[0])):  # 3\n",
    "            for j in range(len(self.w1)):  # 2\n",
    "                self.w1[j][i] = random.gauss(0, 0.5)\n",
    "        for i in range(len(self.w2[0])):  # 3\n",
    "            for j in range(len(self.w2)):  # 2\n",
    "                self.w2[j][i] = random.gauss(0, 0.5)   \n",
    "    \n",
    "    def predict(self, x, training=False):\n",
    "        z1 = [0, 0, 0]\n",
    "        for i in range(len(self.w1[0])):  # 3\n",
    "            for j in range(len(x)):  # 2\n",
    "                z1[i] += x[j]*self.w1[j][i]\n",
    "            z1[i] += self.b1[i]\n",
    "\n",
    "        a1 = [0, 0, 0]\n",
    "        for i, z in enumerate(z1):  # 3\n",
    "            a1[i] = (1/(1+math.exp(-z)))\n",
    "\n",
    "        z2 = [0, 0]\n",
    "        for i in range(len(self.w2[0])):  # 2\n",
    "            for j in range(len(a1)):  # 3\n",
    "                z2[i] += a1[j]*self.w2[j][i]\n",
    "            z2[i] +=  self.b2[i]\n",
    "\n",
    "        exp_a2 = [0, 0]\n",
    "        for i, z in enumerate(z2):  # 2\n",
    "            exp_a2[i] = math.exp(z)\n",
    "\n",
    "        a2 = [0, 0]\n",
    "        sum_ = sum(exp_a2)\n",
    "        for i, exp_a in enumerate(exp_a2):  # 2\n",
    "            a2[i] = exp_a/sum_\n",
    "\n",
    "        if training:\n",
    "            return a2, a1\n",
    "        else:\n",
    "            return a2\n",
    "    \n",
    "    def compute_loss(self, a2, y, training=False):\n",
    "        y_ = [0, 0]\n",
    "        y_[y] = 1\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for i in range(len(a2)):  # 2\n",
    "            if (a2[i] != 0) and (a2[i] != 1):\n",
    "                loss -= (y_[i]*math.log(a2[i]))# + (1 - y_[i])*math.log(1-a2[i]))\n",
    "\n",
    "        dz2 = [0, 0]\n",
    "        for i in range(len(dz2)):  # 2\n",
    "            dz2[i] = a2[i] - y_[i]\n",
    "\n",
    "        if training:\n",
    "            return loss, dz2\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "    def backward_pass(self, dz2, a1, x):\n",
    "        da1 = [0, 0, 0]\n",
    "        dw2 = [[0, 0],\n",
    "               [0, 0],\n",
    "               [0, 0]]\n",
    "        db2 = [0, 0]\n",
    "        for i, dz in enumerate(dz2):  # 2\n",
    "            db2[i] = dz\n",
    "            for j in range(len(a1)): # 3\n",
    "                dw2[j][i] = dz*a1[j]\n",
    "                da1[j] += dz*self.w2[j][i]\n",
    "\n",
    "        dz1 = [0, 0, 0]\n",
    "        for i in range(len(dz1)):  # 3\n",
    "            dz1[i] = da1[i]*a1[i]*(1 - a1[i])\n",
    "\n",
    "        dw1 = [[0, 0, 0],\n",
    "               [0, 0, 0]]\n",
    "        db1 = [0, 0, 0]\n",
    "        for i in range(len(dw1[0])):  # 3\n",
    "            db1[i] = dz1[i]\n",
    "            for j in range(len(dw1)):  # 2\n",
    "                dw1[j][i] = dz1[i]*x[j]\n",
    "\n",
    "        return dw2, db2, dw1, db1\n",
    "\n",
    "    def update_weights(self, dw1, db1, dw2, db2):\n",
    "        for i in range(len(self.w1[0])):\n",
    "            for j in range(len(self.w1)):\n",
    "                self.w1[j][i] -= self.lr*dw1[j][i]\n",
    "\n",
    "        for i in range(len(self.w2[0])):\n",
    "            for j in range(len(self.w2)):\n",
    "                self.w2[j][i] -= self.lr*dw2[j][i]\n",
    "\n",
    "        for i in range(len(self.b1)):\n",
    "            self.b1[i] -= self.lr*db1[i]\n",
    "\n",
    "        for i in range(len(self.b2)):\n",
    "            self.b2[i] -= self.lr*db2[i]\n",
    "            \n",
    "    def train(self, X_train, Y_train, X_val = None, Y_val = None, learning_rate = 0.05, epochs = 5):\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.initialize_weights()\n",
    "\n",
    "        loss_hist = {'train': [], 'val': []}\n",
    "        \n",
    "        for j in range(self.epochs): #epochs\n",
    "\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            X_train, Y_train = X_train[indices], Y_train[indices]  # shuffle training set\n",
    "\n",
    "            for i, (x, y) in enumerate(zip(X_train[indices], Y_train[indices])): # go through all data\n",
    "                a2, a1 = self.predict(x, training=True)\n",
    "                loss, dz2 = self.compute_loss(a2, y, training=True)\n",
    "                dw2, db2, dw1, db1 = self.backward_pass(dz2, a1, x)\n",
    "                self.update_weights(dw1, db1, dw2, db2)\n",
    "\n",
    "            loss_train = 0            \n",
    "            for i, (x, y) in enumerate(zip(X_train[indices], Y_train[indices])): # go through all data\n",
    "                a2 = self.predict(x)\n",
    "                loss = self.compute_loss(a2, y)\n",
    "                loss_train += (1/X_train.shape[0])*loss\n",
    "            loss_hist['train'].append(loss_train)\n",
    "            print(j, 'train:', round(loss_hist['train'][-1], 4))\n",
    "\n",
    "            if X_val is not None:\n",
    "                loss_val = 0\n",
    "                for i, (x, y) in enumerate(zip(X_val, Y_val)):\n",
    "                    a2 = self.predict(x)\n",
    "                    loss = self.compute_loss(a2, y)\n",
    "                    loss_val += (1/X_val.shape[0])*loss\n",
    "                loss_hist['val'].append(loss_val)\n",
    "                print('\\tval:', round(loss_hist['val'][-1], 4))\n",
    "\n",
    "        if X_val is not None:\n",
    "            return loss_hist\n",
    "        else:\n",
    "            return loss_hist['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Neural_net()\n",
    "nn.w1 = [[1, 1, 1],\n",
    "     [-1, -1, -1]]\n",
    "nn.b1 = [0, 0, 0]\n",
    "nn.w2 = [[1, 1],\n",
    "     [-1, -1],\n",
    "     [-1, -1]]\n",
    "nn.b2 = [0, 0]\n",
    "\n",
    "\n",
    "x = [1, -1]\n",
    "\n",
    "a2, a1 = nn.predict(x, training=True)\n",
    "loss, dz2 = nn.compute_loss(a2, 0, training=True)\n",
    "dw2, db2, dw1, db1 = nn.backward_pass(dz2, a1, x)\n",
    "\n",
    "\n",
    "print(loss)\n",
    "print(dw1, db1)\n",
    "print(dw2, db2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hists = {'train': [], 'val': []}\n",
    "\n",
    "for i in range(5):\n",
    "    (x_train, y_train), (x_val, y_val), input_shape = load_synth()\n",
    "    \n",
    "    nn = Neural_net()\n",
    "    loss_hist = nn.train(x_train, y_train, x_val, y_val, epochs=15, learning_rate=1e-3)\n",
    "    hists['train'].append(loss_hist['train'])\n",
    "    hists['val'].append(loss_hist['val'])\n",
    "    print('###########')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtr = np.asarray(hists['train']).mean(axis=0)\n",
    "sdtr = np.asarray(hists['train']).std(axis=0)\n",
    "mte = np.asarray(hists['val']).mean(axis=0)\n",
    "sdte = np.asarray(hists['val']).std(axis=0)\n",
    "\n",
    "t = np.arange(1, mtr.shape[0]+1)\n",
    "\n",
    "plt.plot(t, mtr, lw=2, label='Train', color='blue')\n",
    "plt.plot(t, mte, lw=2, label='Val', color='red')\n",
    "plt.fill_between(t, mtr+sdtr, mtr-sdtr, facecolor='blue', alpha=0.3)\n",
    "plt.fill_between(t, mte+sdte, mte-sdte, facecolor='red', alpha=0.3)\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig('Question 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "class Neural_net:\n",
    "    '''\n",
    "    Neural network with 784 inputs, \n",
    "    1 hidden layer with 300 nodes and sigmoid activation, \n",
    "    an output layer with 10 nodes and softmax activation.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lr = 0.05\n",
    "        self.epochs = 5\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        self.w1 = np.random.randn(300, 784)*0.5\n",
    "        self.b1 = np.zeros((300, 1))\n",
    "        self.w2 = np.random.randn(10, 300)*0.5\n",
    "        self.b2 = np.zeros((10, 1))\n",
    "\n",
    "    def predict(self, x, training=False):\n",
    "        z1 = self.w1.dot(x) + self.b1\n",
    "        a1 = 1 / (1 + np.exp(-z1))\n",
    "        z2 = self.w2.dot(a1) + self.b2\n",
    "        a2 = np.exp(z2)/np.exp(z2).sum(axis=0)\n",
    "        \n",
    "        if training:\n",
    "            return a2, a1\n",
    "        else:\n",
    "            return a2\n",
    "\n",
    "    def compute_loss(self, a2, y_, training=False):\n",
    "        if type(y_) == np.uint8:\n",
    "            m = 1\n",
    "        else:\n",
    "            m = y_.shape[0]\n",
    "\n",
    "        y = np.zeros((m, 10))\n",
    "        y[np.arange(y_.size), y_] = 1\n",
    "        y = y.T         \n",
    "        loss = -(1/m)*np.sum(np.multiply(np.log(a2), y) + np.multiply(np.log(1-a2), (1-y)))\n",
    "        dz2 = a2 - y \n",
    "        \n",
    "        if training:\n",
    "            return loss, dz2\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "    def backward_pass(self, dz2, a1, x):\n",
    "        m = x.shape[1]\n",
    "        dw2 = (1/m)*np.dot(dz2, a1.T)\n",
    "        db2 = (1/m)*np.sum(dz2)\n",
    "        dz1 = (1/m)*np.multiply((a1 * (1 - a1)), np.dot(self.w2.T, dz2))\n",
    "        dw1 = (1/m)*np.dot(dz1, x.T)\n",
    "        db1 = (1/m)*np.sum(dz1)\n",
    "        return dw2, db2, dw1, db1\n",
    "\n",
    "    def update_weights(self, dw1, db1, dw2, db2):\n",
    "        self.w1 -= self.lr*dw1\n",
    "        self.b1 -= self.lr*db1\n",
    "        self.w2 -= self.lr*dw2\n",
    "        self.b2 -= self.lr*db2\n",
    "        \n",
    "    def train(self, X_train, Y_train, X_val = None, Y_val = None, learning_rate = 0.05, epochs = 5):\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.initialize_weights()\n",
    "\n",
    "        loss_hist = {'train': [], 'val': []}\n",
    "        \n",
    "        for j in range(self.epochs): #epochs\n",
    "\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            X_train, Y_train = X_train[indices], Y_train[indices]  # shuffle training set\n",
    "\n",
    "            for i, (x, y) in enumerate(zip(X_train[indices], Y_train[indices])): # go through all data\n",
    "                x = x[:, None]\n",
    "                a2, a1 = self.predict(x, training=True)\n",
    "                loss, dz2 = self.compute_loss(a2, y, training=True)\n",
    "                dw2, db2, dw1, db1 = self.backward_pass(dz2, a1, x)\n",
    "                self.update_weights(dw1, db1, dw2, db2)\n",
    "\n",
    "            preds = self.predict(X_train.T)\n",
    "            loss = self.compute_loss(preds, Y_train)\n",
    "            loss_hist['train'].append(loss)\n",
    "            print(j, loss)\n",
    "\n",
    "            if X_val is not None:\n",
    "                preds = self.predict(X_val.T)\n",
    "                loss = self.compute_loss(preds, Y_val)\n",
    "                loss_hist['val'].append(loss)\n",
    "                print('\\t', loss)\n",
    "                \n",
    "        if X_val is not None:\n",
    "            return loss_hist\n",
    "        else:\n",
    "            return loss_hist['train']        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_v2():\n",
    "    (x_train, y_train), (x_val, y_val), input_shape = load_mnist(final=False)\n",
    "\n",
    "    x_train = x_train / 255\n",
    "    x_val = x_val / 255\n",
    "    \n",
    "    return (x_train, y_train), (x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q5_learning_rates(n_its = 3):\n",
    "    print('LR: ', 0.05)\n",
    "    hists_05 = {'train': [], 'val': []}\n",
    "    for i in range(n_its):\n",
    "        (x_train, y_train), (x_val, y_val) = load_mnist_v2()\n",
    "        nn = Neural_net()\n",
    "        loss_hist = nn.train(x_train, y_train, x_val, y_val, epochs=5, learning_rate=0.05)\n",
    "        hists_05['train'].append(loss_hist['train'])\n",
    "        hists_05['val'].append(loss_hist['val'])\n",
    "        print()\n",
    "     \n",
    "    print('LR: ', 0.01)\n",
    "    hists_01 = {'train': [], 'val': []}\n",
    "    for i in range(n_its):\n",
    "        (x_train, y_train), (x_val, y_val) = load_mnist_v2()\n",
    "        nn = Neural_net()\n",
    "        loss_hist = nn,traub(x_train, y_train, x_val, y_val, epochs=5, learning_rate=0.01)\n",
    "        hists_01['train'].append(loss_hist['train'])\n",
    "        hists_01['val'].append(loss_hist['val'])\n",
    "        print()\n",
    "    \n",
    "    print('LR: ', 0.001)\n",
    "    hists_001 = {'train': [], 'val': []}\n",
    "    for i in range(n_its):\n",
    "        (x_train, y_train), (x_val, y_val) = load_mnist_v2()\n",
    "        nn = Neural_net()\n",
    "        loss_hist = nn.train(x_train, y_train, x_val, y_val, epochs=5, learning_rate=0.001)\n",
    "        hists_001['train'].append(loss_hist['train'])\n",
    "        hists_001['val'].append(loss_hist['val'])\n",
    "        print()\n",
    "        \n",
    "    return hists_05, hists_01, hists_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hists_05, hists_01, hists_001 = q5_learning_rates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtr05 = np.asarray(hists_05['train']).mean(axis=0)\n",
    "sdtr05 = np.asarray(hists_05['train']).std(axis=0)\n",
    "mte05 = np.asarray(hists_05['val']).mean(axis=0)\n",
    "sdte05 = np.asarray(hists_05['val']).std(axis=0)\n",
    "\n",
    "mtr01 = np.asarray(hists_01['train']).mean(axis=0)\n",
    "sdtr01 = np.asarray(hists_01['train']).std(axis=0)\n",
    "mte01 = np.asarray(hists_01['val']).mean(axis=0)\n",
    "sdte01 = np.asarray(hists_01['val']).std(axis=0)\n",
    "\n",
    "mtr001 = np.asarray(hists_001['train']).mean(axis=0)\n",
    "sdtr001 = np.asarray(hists_001['train']).std(axis=0)\n",
    "mte001 = np.asarray(hists_001['val']).mean(axis=0)\n",
    "sdte001 = np.asarray(hists_001['val']).std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(1, 6)\n",
    "fig, (ax0, ax1, ax2) = plt.subplots(nrows=1, ncols=3, sharex=True, sharey=True, figsize=(12, 6))\n",
    "\n",
    "ax0.plot(t, mtr05, lw=2, label='Train', color='blue')\n",
    "ax0.plot(t, mte05, lw=2, label='Val', color='red')\n",
    "ax0.fill_between(t, mtr05+sdtr05, mtr05-sdtr05, facecolor='blue', alpha=0.5)\n",
    "ax0.fill_between(t, mte05+sdte05, mte05-sdte05, facecolor='red', alpha=0.5)\n",
    "ax0.set_title('LR: 0.05')\n",
    "\n",
    "ax1.plot(t, mtr01, lw=2, label='Train', color='blue')\n",
    "ax1.plot(t, mte01, lw=2, label='Val', color='red')\n",
    "ax1.fill_between(t, mtr01+sdtr01, mtr01-sdtr01, facecolor='blue', alpha=0.5)\n",
    "ax1.fill_between(t, mte01+sdte01, mte01-sdte01, facecolor='red', alpha=0.5)\n",
    "ax1.set_title(r'LR: 0.01')\n",
    "\n",
    "ax2.plot(t, mtr001, lw=2, label='Train', color='blue')\n",
    "ax2.plot(t, mte001, lw=2, label='Val', color='red')\n",
    "ax2.fill_between(t, mtr001+sdtr001, mtr001-sdtr001, facecolor='blue', alpha=0.5)\n",
    "ax2.fill_between(t, mte001+sdte001, mte001-sdte001, facecolor='red', alpha=0.5)\n",
    "ax2.set_title(r'LR: 0.001')\n",
    "\n",
    "ax2.legend(loc='upper right')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax0.set_ylabel('Cross Entropy Loss')\n",
    "ax0.grid()\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "\n",
    "fig.suptitle('Effect of Learning Rate on Training and Validation Loss')\n",
    "plt.savefig('Question 5')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5-final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "(x_train, y_train), (x_test, y_test), input_shape = load_mnist(final=True)\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "    \n",
    "nn = Neural_net()\n",
    "training_loss = nn.train(x_train, y_train, x_test, y_test, epochs=5, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = nn.predict(x_test.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.asarray(pd.DataFrame(preds.T).idxmax(axis=1))   \n",
    "print('Accuracy:',  accuracy_score(y_test, preds))\n",
    "print('F1 Score:', f1_score(y_test, preds, average='macro'))\n",
    "print('Confusion Matrix:', confusion_matrix(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
